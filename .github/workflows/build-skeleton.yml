name: Update Archive

on:
  push:
    branches: [ master ]
    tags: [ '*' ]

jobs:
  skeleton:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2
    - uses: actions/cache@v1
      with:
        path: /usr/share/miniconda
        key: conda-dist-v1
        restore-keys: |
          conda-dist-
    - name: Setup conda
      uses: s-weigand/setup-conda@v1
      with:
        update-conda: false
        python-version: 3.7
        conda-channels: anaconda, conda-forge
    - name: Install dependencies
      run: |
        conda config --set always_yes True
        conda install git-annex datalad osfclient
    - name: Pacify DataLad about git config
      run: |
        git config --global user.name "GitHub Action"
        git config --global user.email "nipreps@gmail.com"
    - name: Install all datasets
      run: |
        datalad install -r tpl-*/
    - name: Generate skeleton
      run: |
        find . -type d -not -path "*/\.*" -exec mkdir -p /tmp/skel/{} \;
        for i in $( find . -type f -not -path "*/\.*" ); do cp $i /tmp/skel/${i:2}; done
        find . -type l -not -path "*/\.*" -exec touch /tmp/skel/{} \;
        cd /tmp/skel
        mkdir -p /tmp/resources
        zip -r /tmp/resources/templateflow-skel.zip */
        find . -type f -exec md5sum {} \; | sort -k 2 | md5sum > /tmp/resources/templateflow-skel.md5
    - name: Save new skeleton
      uses: actions/upload-artifact@v1
      with:
        name: templateflow-skel
        path: /tmp/resources/templateflow-skel.zip
    - name: Save new skeleton checksum
      uses: actions/upload-artifact@v1
      with:
        name: templateflow-skel.md5
        path: /tmp/resources/templateflow-skel.md5
    - name: Upload skeleton to OSF
      shell: bash
      env:
        OSF_PASSWORD: ${{ secrets.OSF_PASSWORD }}
        OSF_USERNAME: ${{ secrets.OSF_USERNAME }}
      run: |
        cd /tmp/resources
        if [[ "$GITHUB_REF" == refs/tags/* ]]; then
          TAG=${GITHUB_REF##*/}
        fi
        SHORTSHA=${GITHUB_SHA::8}
        FILENAME=templateflow-${TAG:-latest}
        echo "Uploading new skeleton: $FILENAME"
        osf -u "$OSF_USERNAME" -p ue5gx upload -f templateflow-skel.zip .skel/${FILENAME}.zip
        osf -u "$OSF_USERNAME" -p ue5gx upload -f templateflow-skel.md5 .skel/${FILENAME}.md5
        
    - name: Update skeleton within the Python client
      uses: actions/checkout@v2
      with:
        repository: templateflow/python-client
        path: ~/python-client
    - name: Overwrite skeleton
      run: |
        cd ~/python-client
        cp /tmp/resources/templateflow-skel.{zip,md5} ~/python-client/templateflow/conf/
        git status
        
  export:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2
    - uses: actions/cache@v1
      with:
        path: /usr/share/miniconda
        key: conda-dist-v1
        restore-keys: |
          conda-dist-
    - name: Setup conda
      uses: s-weigand/setup-conda@v1
      with:
        update-conda: false
        python-version: 3.7
        conda-channels: anaconda, conda-forge
    - name: Install dependencies
      run: |
        conda config --set always_yes True
        conda install git-annex datalad osfclient
    - name: Pacify DataLad about git config
      run: |
        git config --global user.name "GitHub Action"
        git config --global user.email "nipreps@gmail.com"
    - name: Install all datasets
      run: |
        datalad install -r tpl-*/
        find /home/runner/work/templateflow/templateflow/ -maxdepth 1 -name "tpl-*" -type d \
            -exec datalad siblings -d "{}" enable -s public-s3 \;
    - name: Export to S3
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        for tpl in $( find /home/runner/work/templateflow/templateflow/ -maxdepth 1 -name "tpl-*" -type d ); do
          pushd $tpl
          datalad get -r .
          git annex export master --to public-s3
          popd
        done
